{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2ha9OWxf0jw"
   },
   "source": [
    "DS Build Week Project\n",
    "\n",
    "*Tanzania Waterpumps prediction*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-TExplb_Slf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "DATA_PATH = '../notebooks/waterpumps/'\n",
    "# Merge train_features.csv & train_labels.csv\n",
    "train = pd.merge(pd.read_csv(DATA_PATH+'train_features.csv'), \n",
    "                 pd.read_csv(DATA_PATH+'train_labels.csv'))\n",
    "\n",
    "# Read test_features.csv & sample_submission.csv\n",
    "test = pd.read_csv(DATA_PATH+'test_features.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH+'sample_submission.csv')\n",
    "\n",
    "\n",
    "# Split train into train & val\n",
    "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
    "                              stratify=train['status_group'], random_state=42)\n",
    "\n",
    "\n",
    "def wrangle(X):\n",
    "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
    "    \n",
    "    # Prevent SettingWithCopyWarning\n",
    "    X = X.copy()\n",
    "    \n",
    "    # About 3% of the time, latitude has small values near zero,\n",
    "    # outside Tanzania, so we'll treat these values like zero.\n",
    "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
    "    \n",
    "    # When columns have zeros and shouldn't, they are like null values.\n",
    "    # So we will replace the zeros with nulls, and impute missing values later.\n",
    "    # Also create a \"missing indicator\" column, because the fact that\n",
    "    # values are missing may be a predictive signal.\n",
    "    cols_with_zeros = ['longitude', 'latitude', 'construction_year', \n",
    "                       'gps_height', 'population']\n",
    "    for col in cols_with_zeros:\n",
    "        X[col] = X[col].replace(0, np.nan)\n",
    "        X[col+'_MISSING'] = X[col].isnull()\n",
    "            \n",
    "    # Drop duplicate columns\n",
    "    duplicates = ['quantity_group', 'payment_type']\n",
    "    X = X.drop(columns=duplicates)\n",
    "    \n",
    "    # Drop recorded_by (never varies) and id (always varies, random)\n",
    "    unusable_variance = ['recorded_by', 'id']\n",
    "    X = X.drop(columns=unusable_variance)\n",
    "    \n",
    "    # Convert date_recorded to datetime\n",
    "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
    "    \n",
    "    # Extract components from date_recorded, then drop the original column\n",
    "    X['year_recorded'] = X['date_recorded'].dt.year\n",
    "    X['month_recorded'] = X['date_recorded'].dt.month\n",
    "    X['day_recorded'] = X['date_recorded'].dt.day\n",
    "    X = X.drop(columns='date_recorded')\n",
    "    \n",
    "    # Engineer feature: how many years from construction_year to date_recorded\n",
    "    X['years'] = X['year_recorded'] - X['construction_year']\n",
    "    X['years_MISSING'] = X['years'].isnull()\n",
    "    \n",
    "    # return the wrangled dataframe\n",
    "    return X\n",
    "\n",
    "train = wrangle(train)\n",
    "val = wrangle(val)\n",
    "test = wrangle(test)"
   ]
  },
  {
   "source": [
    "### Save a .csv file for visulization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['amount_tsh', 'funder', 'gps_height', 'installer', 'longitude',\n",
       "       'latitude', 'wpt_name', 'num_private', 'basin', 'subvillage', 'region',\n",
       "       'region_code', 'district_code', 'lga', 'ward', 'population',\n",
       "       'public_meeting', 'scheme_management', 'scheme_name', 'permit',\n",
       "       'construction_year', 'extraction_type', 'extraction_type_group',\n",
       "       'extraction_type_class', 'management', 'management_group', 'payment',\n",
       "       'water_quality', 'quality_group', 'quantity', 'source', 'source_type',\n",
       "       'source_class', 'waterpoint_type', 'waterpoint_type_group',\n",
       "       'status_group', 'longitude_MISSING', 'latitude_MISSING',\n",
       "       'construction_year_MISSING', 'gps_height_MISSING', 'population_MISSING',\n",
       "       'year_recorded', 'month_recorded', 'day_recorded', 'years',\n",
       "       'years_MISSING'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 47520 entries, 43360 to 49783\nData columns (total 46 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   amount_tsh                 47520 non-null  float64\n 1   funder                     44616 non-null  object \n 2   gps_height                 31215 non-null  float64\n 3   installer                  44603 non-null  object \n 4   longitude                  46078 non-null  float64\n 5   latitude                   46078 non-null  float64\n 6   wpt_name                   47520 non-null  object \n 7   num_private                47520 non-null  int64  \n 8   basin                      47520 non-null  object \n 9   subvillage                 47234 non-null  object \n 10  region                     47520 non-null  object \n 11  region_code                47520 non-null  int64  \n 12  district_code              47520 non-null  int64  \n 13  lga                        47520 non-null  object \n 14  ward                       47520 non-null  object \n 15  population                 30454 non-null  float64\n 16  public_meeting             44876 non-null  object \n 17  scheme_management          44392 non-null  object \n 18  scheme_name                24988 non-null  object \n 19  permit                     45077 non-null  object \n 20  construction_year          31003 non-null  float64\n 21  extraction_type            47520 non-null  object \n 22  extraction_type_group      47520 non-null  object \n 23  extraction_type_class      47520 non-null  object \n 24  management                 47520 non-null  object \n 25  management_group           47520 non-null  object \n 26  payment                    47520 non-null  object \n 27  water_quality              47520 non-null  object \n 28  quality_group              47520 non-null  object \n 29  quantity                   47520 non-null  object \n 30  source                     47520 non-null  object \n 31  source_type                47520 non-null  object \n 32  source_class               47520 non-null  object \n 33  waterpoint_type            47520 non-null  object \n 34  waterpoint_type_group      47520 non-null  object \n 35  status_group               47520 non-null  object \n 36  longitude_MISSING          47520 non-null  bool   \n 37  latitude_MISSING           47520 non-null  bool   \n 38  construction_year_MISSING  47520 non-null  bool   \n 39  gps_height_MISSING         47520 non-null  bool   \n 40  population_MISSING         47520 non-null  bool   \n 41  year_recorded              47520 non-null  int64  \n 42  month_recorded             47520 non-null  int64  \n 43  day_recorded               47520 non-null  int64  \n 44  years                      31003 non-null  float64\n 45  years_MISSING              47520 non-null  bool   \ndtypes: bool(6), float64(7), int64(6), object(27)\nmemory usage: 16.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhg8PQKt_jzP"
   },
   "outputs": [],
   "source": [
    "# Arrange data into X features matrix and y target vector\n",
    "target = 'status_group'\n",
    "X_train = train.drop(columns=target)\n",
    "y_train = train[target]\n",
    "X_val = val.drop(columns=target)\n",
    "y_val = val[target]\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m8lB4z5l_eml"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation Accuracy 0.8135521885521886\n"
     ]
    }
   ],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median'), \n",
    "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "\n",
    "# Fit on train, score on val\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))"
   ]
  },
  {
   "source": [
    "### Use permutation importance to find the top 5 most important features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpSemTkFFP8i"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "r = permutation_importance(pipeline, X_val, y_val,\n",
    "                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame({'columns':X_val.columns, 'feature_importance':r.importances_mean}).sort_values(by='feature_importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_filtered = df_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = df_features_filtered['columns'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['quantity',\n",
       " 'amount_tsh',\n",
       " 'waterpoint_type',\n",
       " 'extraction_type_class',\n",
       " 'longitude']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "feature_list "
   ]
  },
  {
   "source": [
    "### Use these 5 new features to train the model again!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation Accuracy 0.7134680134680135\n"
     ]
    }
   ],
   "source": [
    "X_train_feature_filtered = X_train[feature_list]\n",
    "X_val_feature_filtered = X_val[feature_list]\n",
    "pipeline_filtered = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median'), \n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    ")\n",
    "\n",
    "# Fit on train, score on val\n",
    "pipeline_filtered.fit(X_train_feature_filtered, y_train)\n",
    "print('Validation Accuracy', pipeline_filtered.score(X_val_feature_filtered, y_val))"
   ]
  },
  {
   "source": [
    "### Save your model in .joblib"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['pipeline.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(pipeline_filtered, 'pipeline.joblib', compress=True)"
   ]
  },
  {
   "source": [
    "###  check the description of the continuous variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_cols =  X_train_feature_filtered.select_dtypes(exclude='object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "count     47520.000000\nmean        321.925261\nstd        3197.240487\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%          25.000000\nmax      350000.000000\nName: amount_tsh, dtype: float64\ncount    46078.000000\nmean        35.149033\nstd          2.604241\nmin         29.607122\n25%         33.284679\n50%         35.008578\n75%         37.223501\nmax         40.344301\nName: longitude, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for con in con_cols:\n",
    "    print( X_train_feature_filtered[con].describe())"
   ]
  },
  {
   "source": [
    "###  check the unique values in each of the continous variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = X_train_feature_filtered.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "quantity\n['insufficient', 'enough', 'dry', 'seasonal', 'unknown']\nwaterpoint_type\n['communal standpipe', 'hand pump', 'other', 'communal standpipe multiple', 'improved spring', 'cattle trough', 'dam']\nextraction_type_class\n['gravity', 'handpump', 'other', 'motorpump', 'submersible', 'rope pump', 'wind-powered']\n"
     ]
    }
   ],
   "source": [
    "for cat in cat_cols:\n",
    "    print(cat)\n",
    "    print(X_train_feature_filtered[cat].unique().tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('waterpump_app_ds24': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "4eb519cec39c0aaa1f99db08a29efb1a4a4024f28d1ec354c16402233db7003b"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}